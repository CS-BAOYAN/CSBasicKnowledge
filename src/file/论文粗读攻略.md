## 原始积累

首先先积累一些基础知识，比如说一些经典的网络架构，比如说经典的 transformer，然后以及各个大的领域主要 focus 的东西，包括说一些这方面比较经典的绝活，诸如对比学习里的那个 InstDisc 等等。

## 认祖归宗

首先问一问老师，看看自己做的这个领域有没有经典文章，也就是一些老祖宗级别的 baseline，比如说我有读过 mean teacher，在半监督领域挺知名的，这类论文的一个特点是，你去最新的论文的 introduction 里面转一圈，基本还会提到它们。

这些论文的思想都是十分实用的，了解这些论文算是进阶版的知识积累。一个“稳啦”的开局是，有大佬做过相关的综述或者论文串讲，别找比较野鸡的，对于方法的故事后面真正 work 的东西是什么有了解很重要。

## GPT，启动！

接下来你终于来到了论文粗度的起点，所以说还不快去注册一个 gpt，为了以防有打广告的嫌疑，仅是提一嘴，假如觉得注册很麻烦，买一个号很方便的，几块钱就够，被封了大不了再买一个。

一般来说，我们认为中文的知识密度是大于英文的，而且作为母语，也比较好理解。

直接全部复制摘要，问 GPT，翻译以下内容，然后你就知道它干了什么。甚至因为排版的问题，可能你直接复制会有一定乱的格式，都不用管，gpt can handle this。摘要是一篇文章的精华，告诉了你他们的 work 是什么。

推荐在看introduction之前再看看论文的附图（一般的cs论文都会有附图的，无论是模型结构图还是性能比较图亦或是采集数据集的实地照片或者构建的3D模型的图），都可以帮助阅读者快速了解论文作者到底在干什么，顺便快速判断一篇论文是否有阅读的价值。

之后看 introduction，作为一个领域刚刚入门的人，可以进行一个迭代学习，看到不会的之前的工作，向前回溯，introduction 是论文的故事在的地方，看看可以知道这篇工作的故事走向，可以信一部分，但别完全信。

之后跳到 discussions，老规矩继续 gpt 翻译，不会的东西先问 gpt，再查网上，毕竟都到论文了，网上的东西只会越来越少，尤其是一些下游任务，和它聊久了你就能感觉出来它有什么东西讲的很确信，有什么是在瞎扯。

这时候你已经读完论文了，你可以大言不惭的说你读完了，但是假如说你觉得这个工作很吸引你，那就继续看，related works 就是一个小综述，看看总没坏处，万一写得很好呢。

之后 method 里面别管方法的名字，看这些公式的本质，实在不行问问 gpt 或者去查查，说不定这个方法只是之前的一个别的方法的翻版，毕竟 MLP 都可以叫做 predictor，方法的名字取决于故事，但是它为什么 work，你可能需要自己想一想自己的理解，捋一下数据在它的 pipeline 图里的流向。

在之后，在 experiment 里面主要关注消融实验，方法对比主要是秀肌肉，消融实验能告诉你它可能提出的三五种策略组合起来之后，哪些 work 了。

这样你就彻底读完一篇论文了，它具体有没有前景你的心里也已经有数了。

## 润一下源码

接下来你要是想要去做，和老师谈一谈，然后找到它的代码，作为新手，还是建议大家找有源码的论文，在上面做拓展也方便，不懂的地方依然 gpt 解决，直接每个文件 `ctrl+A` 全部复制，或者分段复制，让 gpt 加中文注释或者简要解释每个函数的功能，之后 `tree` 一下，也给 gpt，方便 gpt 理解一切。

潤码时有一个不错的网站推荐：Paper with Code，里面的论文基本上都会附带GitHub的仓库链接，可以快速找到与论文相关的代码。
\[[Main Page](https://paperswithcode.com/)\]

代码也是讲究一个跑通就好，看看上面的 args 里面都有啥，然后看看 dataloader 这部分的循环里面怎么训练的，假如有实现一个 module，看看里面的结构，像是数据处理之类的就不用管了，主要看 idea 是怎么实现的，至于怎么读，这里我要提一个人类的好朋友，gpt......

以上，当然，也不难发现，最重要的是成为一个 gpt chater，每一个人一天和 gpt 说话少于 20 句，都是摸鱼的一天。

Version 0.0.1